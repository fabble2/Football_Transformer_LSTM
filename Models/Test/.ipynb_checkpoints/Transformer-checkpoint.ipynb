{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from itertools import chain \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------DATENVERARBEITUNG---------------------------------------------\n",
    "\n",
    "\n",
    "#input: Spieltagsnummer, der vorhergesagt werden soll\n",
    "#von jeder Mannschaft die Daten vom Spiel holen\n",
    "#Erstellung von Trainings- und Testdaten\n",
    "def getdata(nummer):\n",
    "    data=pd.read_pickle(\"Daten2020.pkl\")\n",
    "    matrix= data[0]\n",
    "    mannschaften= data[2]\n",
    "    schleifennummer= nummer-1\n",
    "    training=[]\n",
    "    test=[]\n",
    "    \n",
    "    #Alle Spiele vom 1. Spieltag bis zum x. Spieltag holen\n",
    "    #in Trainings und Testdaten zuordnen\n",
    "    for mannschaft in range(0,18):\n",
    "        for spieltag in range(0,schleifennummer):\n",
    "            training.append(matrix[mannschaft][spieltag])\n",
    "        test.append(matrix[mannschaft][nummer-1])\n",
    "    \n",
    "    #Spiele der Mannschaften zuordnen, ersten x Spiele gehören Team 1 usw.\n",
    "    training_dic={}\n",
    "    for i in range(0,18):\n",
    "        training_dic[i]= training[(nummer-1)*i:(nummer-1)*(i+1)]\n",
    "    \n",
    "    return training_dic,test,mannschaften\n",
    "\n",
    "\n",
    "#Hilfsmethode um Anzahl der Spiele und time_size zu bestimmen\n",
    "#input: Gewünschter Spieltag, der vorhergesagt werden soll\n",
    "#gewollt ist eine Betrachtung von 5 Spielen von 5 Spielen\n",
    "#wenn dies nicht geht schaut man sich weniger Spiele für ein Spiel an\n",
    "#worst-case: keine 5 Spiele betrachtbar und dafür auch keine 5 Spiele, sondern 2 und 1\n",
    "#output: time_size, anzahl\n",
    "\n",
    "def size_of_time(spieltag):\n",
    "    \n",
    "    if spieltag >12:\n",
    "        time_size= 5\n",
    "        anzahl= spieltag-6\n",
    "    else:\n",
    "        #spieltag=spieltag-1\n",
    "        differenz1= spieltag-6\n",
    "        differenz2= spieltag-5\n",
    "        if differenz1 >=3: #Spieltag 9,10,11\n",
    "            time_size= differenz1\n",
    "            anzahl=5\n",
    "        elif differenz2 ==3: #Spieltag 8\n",
    "            time_size= differenz2\n",
    "            anzahl=4\n",
    "        elif spieltag-4==3:# Spieltag 7\n",
    "            time_size= 3\n",
    "            anzahl= 3\n",
    "        elif spieltag-3==3: # Spieltag 6\n",
    "            time_size= 2\n",
    "            anzahl= 3\n",
    "        elif spieltag-2==3: #Spieltag 5\n",
    "            time_size= 3\n",
    "            anzahl= 3\n",
    "        elif spieltag-1==3: # Spieltag 4\n",
    "            time_size= 2\n",
    "            anzahl= 2\n",
    "        elif spieltag >2: #usw.\n",
    "            time_size=1\n",
    "            anzahl=1\n",
    "        else:\n",
    "            time_size=0\n",
    "            anzahl=0\n",
    "    return time_size,anzahl\n",
    "\n",
    "#input; Daten, team\n",
    "#Filterung der Statistiken des jeweiligen Teams\n",
    "#output: Liste der Statistiken von den Spielen\n",
    "def get_statistik(data, team):\n",
    "    liste=[]\n",
    "    \n",
    "    for i in data:\n",
    "        teams= i[-2:]\n",
    "        werte= i[:-2]\n",
    "        if teams[0]==team:\n",
    "            liste.append(werte[19:32])\n",
    "        else:\n",
    "            liste.append(werte[51:64])\n",
    "    return liste\n",
    "\n",
    "#Ähnlichkeitssuche der Mannschaften\n",
    "#Mannschaft wird in Mannschaftsliste gesucht\n",
    "#Damit andere schreibweisen der Mannschaft zur gleichen Mannschaft führen\n",
    "#output: Mannschaft in richitger Schreibform\n",
    "def similar(mannschaft, liste): \n",
    "    wert=0\n",
    "    currentteam=\"\"\n",
    "    \n",
    "    for team in liste:\n",
    "        similarity= SequenceMatcher(None, mannschaft, team).ratio()\n",
    "        #print(team, similarity)\n",
    "        if wert < similarity:\n",
    "            wert= similarity\n",
    "            currentteam= team\n",
    "            \n",
    "    return currentteam\n",
    "\n",
    "#Reduzierung der Daten\n",
    "#Wenn ein Aufsteiger sich beim abfragenen Spiel sich befindet\n",
    "#kann man sich die vorherige Saison nicht anschauen, da dort sich nur eine Mannschaft befindet\n",
    "#Also --> Daten der vorherigen Saison entfernen\n",
    "#Wenn ein Absteiger sich beim abfragenen Spiel sich befindet\n",
    "#--> Fehlermeldung\n",
    "#Dies wird für 3 Saisons gemacht(also 2 Auf-/Absteiger)\n",
    "#output: Daten, neuer Spieltag(anstatt 40, 40-34=6)\n",
    "def data_auf_absteiger(data,mannschaftsliste,mannschaft,spieltag,absteiger,aufsteiger):\n",
    "\n",
    "    #Indices der spielenenden Mannschaften herausfinden\n",
    "    current_team1=similar(mannschaft[0],mannschaftsliste)\n",
    "    index1= mannschaftsliste.index(current_team1)\n",
    "    \n",
    "    current_team2=similar(mannschaft[1],mannschaftsliste)\n",
    "    index2= mannschaftsliste.index(current_team2)\n",
    "    \n",
    "    #Zusammenführung der Auf- bzw. Absteiger in einer Liste für Abfrage\n",
    "    auf= list(chain.from_iterable(list(aufsteiger.values())))\n",
    "    ab= list(chain.from_iterable(list(absteiger.values())))\n",
    "    \n",
    "    if spieltag >34 and (index1 in auf or index2 in auf or index1 in ab or index2 in ab):\n",
    "        \n",
    "        #Saison 2\n",
    "        if spieltag <69:\n",
    "            #Auf-/Absteiger von Saison 3 entfernen\n",
    "            del absteiger[1]\n",
    "            del aufsteiger[2]\n",
    "            #Schauen ob Mannschaft Auf-/Absteiger ist, zweite Saison\n",
    "            \n",
    "            # Wenn sich um Absteiger handelt, ist dieser nicht in aktueller Saison --> Fehler\n",
    "            for key,values in absteiger.items():\n",
    "                if index1 in values or index2 in values:\n",
    "                    return \"KEY ERROR\",\"\"\n",
    "            #Wenn sich um Aufsteiger handelt \n",
    "            #--> Saison 1 aus Daten nehmen und Spieltag aktualisiern\n",
    "            for key,values in aufsteiger.items():\n",
    "                if index1 in values or index2 in values:\n",
    "                    for key, values in data.items():\n",
    "                        data.update({key:values[34:]})\n",
    "                    return data, spieltag-34\n",
    "        #Saison 3\n",
    "        if spieltag >=69:\n",
    "            #Absteiger entfernen\n",
    "            del absteiger[0]\n",
    "            \n",
    "            # Wenn sich um Absteiger handelt, ist dieser nicht in aktueller Saison --> Fehler\n",
    "            for key,values in absteiger.items():\n",
    "                if index1 in values or index2 in values:\n",
    "                    return \"KEY ERROR\",\"\"\n",
    "            #Wenn sich um Aufsteiger handelt \n",
    "            #--> Ab Saison 1 bzw. ab Saison 2 aus Daten nehmen und Spieltag aktualisiern\n",
    "            for key,values in aufsteiger.items():\n",
    "                if index1 in values or index2 in values:\n",
    "                    if key==1: #Aufsteiger von Sasion2\n",
    "                        for key, values in data.items():\n",
    "                            data.update({key:values[34:]})\n",
    "                        return data, spieltag-34\n",
    "                    else: #Aufsteiger von Saison3\n",
    "                        for key, values in data.items():\n",
    "                            data.update({key:values[68:]})\n",
    "                        return data, spieltag-68\n",
    "    else:# Fall für Saison1 ==> keine Reduzierung notwendig\n",
    "        return data, spieltag\n",
    "    \n",
    "#Gegner der übergebenen Mannschaft herausfinden\n",
    "#Daten durchlaufen und Mannschaft finden. Dort befindet sich auch die GegnerID\n",
    "#output: Name des Gegners und Ort(ist Mannschaft Heim oder Gastmannschaft)\n",
    "def getgegner(data,teamname,mannschaftsliste):\n",
    "    current_team=similar(teamname,mannschaftsliste)\n",
    "    key= mannschaftsliste.index(current_team)\n",
    "    \n",
    "    #Gegner finden\n",
    "    for i in data:\n",
    "        teams= i[-2:]\n",
    "        if key in teams:\n",
    "            current_teams= teams\n",
    "            break\n",
    "    #Heim oder Auswärtsmannschaft\n",
    "    if list(current_teams).index(key)==0:\n",
    "        ort= \"home\"\n",
    "    else:\n",
    "        ort= \"away\"\n",
    "    \n",
    "    #Name des Gegner bekommen\n",
    "    gegner= current_teams[current_teams!=key][0]\n",
    "    gegnername= mannschaftsliste[int(gegner)]\n",
    "    return gegnername,ort\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------TIMESEQUENZEN/AUXDATA TRAINING---------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#Time Sequenzen erstellen\n",
    "#Im Besten Fall werden pro Spiel die letzten 5 Spielestatistiken betrachtet\n",
    "#Wie viele betrachtet werden, wird mit size_of_time berechnet\n",
    "#Nun x Statistiken von der Mannschaft holen und dessen Gegner\n",
    "#Diese in einer Liste speichern und den nächsten Spieltag beachten\n",
    "#output: Liste von TimeSequzenen, Größe der einzelnen TimeSequenzen, Mannschaftskey\n",
    "def time_sequence(data,mannschaftsliste,mannschaft,spieltag):\n",
    "    time_size, anzahl= size_of_time(spieltag)\n",
    "    time_series={}\n",
    "    list_of_game_series=[]\n",
    "    grenze= spieltag-1\n",
    "    \n",
    "    #Mannschaftnummer finden\n",
    "    current_team=similar(mannschaft,mannschaftsliste)\n",
    "    #TimeSequenzen erstellen\n",
    "    for key,values in data.items():\n",
    "        if key!= mannschaftsliste.index(current_team):\n",
    "            continue\n",
    "        else:\n",
    "            for i in range(time_size,grenze): #Spiele die betrachtet werden\n",
    "                teams= values[i][-2:]\n",
    "                start_gegner= i-time_size\n",
    "                if teams[0]==key:\n",
    "                    #\"Rohe\" Daten holen\n",
    "                    gegner_statistik= data[teams[1]][start_gegner:i]\n",
    "                    team_statistik= data[teams[0]][start_gegner:i]\n",
    "                    #Filterung der Statistiken\n",
    "                    team_statistik= get_statistik(team_statistik,teams[0])\n",
    "                    gegner_statistik= get_statistik(gegner_statistik,teams[1])\n",
    "                    #Zusammenführung\n",
    "                    game_series= np.concatenate((team_statistik, gegner_statistik), axis=1)\n",
    "                else:\n",
    "                    #\"Rohe\" Daten holen\n",
    "                    gegner_statistik= data[teams[0]][start_gegner:i]\n",
    "                    team_statistik= data[teams[1]][start_gegner:i]              \n",
    "                    #Filterung der Statistiken\n",
    "                    team_statistik= get_statistik(team_statistik,teams[1])\n",
    "                    gegner_statistik= get_statistik(gegner_statistik,teams[0])\n",
    "                    #Zusammenführung\n",
    "                    game_series= np.concatenate((gegner_statistik, team_statistik), axis=1)\n",
    "                list_of_game_series.append(game_series)\n",
    "                \n",
    "    mannschaftskey= mannschaftsliste.index(current_team)\n",
    "    return np.array(list_of_game_series), mannschaftskey, time_size\n",
    "\n",
    "#Auxilary Data erstellen, also Tabellen-,Mannschafts-,Spielerwerte, Wettquoten\n",
    "#Output für diese Spiele auch herausziehen\n",
    "#size of time benutzen um betrachtete Spiele zu finden\n",
    "#Liste für aux_data und output erstellen\n",
    "def auxilary_data(data,mannschaftskey,spieltag):\n",
    "    time_size, anzahl= size_of_time(spieltag)\n",
    "    aux_data={}\n",
    "    out_data={}\n",
    "    grenze= spieltag-1\n",
    "    \n",
    "    for key,values in data.items():\n",
    "        list_of_game_series=[]\n",
    "        list_of_outputs=[]\n",
    "        if key!= mannschaftskey:\n",
    "            continue\n",
    "        else:\n",
    "            for i in range(time_size,grenze):\n",
    "                teams= values[i][-2:]\n",
    "                start_gegner= i-time_size\n",
    "                #Aux_Data und Output holen\n",
    "                home_team= values[i][:19]\n",
    "                away_team= values[i][32:51]\n",
    "                odds= values[i][64:67]\n",
    "                output= values[i][67:70]\n",
    "                game_data= np.concatenate((home_team,away_team,odds))\n",
    "                #Speichern in Listen\n",
    "                list_of_outputs.append(output)\n",
    "                list_of_game_series.append(game_data)\n",
    "            #Separierung der Mannschaften durch dictionary\n",
    "            aux_data[key]=list_of_game_series\n",
    "            out_data[key]=list_of_outputs\n",
    "            \n",
    "    return list(aux_data.values()),list(out_data.values())\n",
    "\n",
    "#Umformung des Outputs in einzelne Werte, [1,0,0]=1,[0,1,0]=0,[0,0,1]=2\n",
    "#output: Array mit einzelenen Werten anstatt der einzelenen Listen\n",
    "def out_data(outputs):\n",
    "    output=[]\n",
    "    \n",
    "    for i in outputs:\n",
    "        if i[0]==1:\n",
    "            output.append(0)\n",
    "        elif i[1]==2:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(2)\n",
    "    return np.array(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------TIMESEQUENZEN/AUXDATA FÜR DEN TEST----------------------------------\n",
    "\n",
    "\n",
    "#TimeSezquenz für die Testdaten erstellen\n",
    "#Unterschied man betrachet nur das eine Spiel\n",
    "#Zunächst Heim- und Auswärtsmannschaft herausfinden\n",
    "#Dann für beide Mannschaften die TimeSequenz erstellen\n",
    "#Schleife läuft nicht durch, da das Spiel sich zweimal in der Liste befindet\n",
    "#output: array mit Timesequenzen\n",
    "def time_sequence_test(data_training,data_test,team,spieltag):\n",
    "\n",
    "    time_size, anzahl= size_of_time(spieltag)\n",
    "    liste=[]\n",
    "    \n",
    "    #Herausfindeen ob Team Heim oder Auswärtsmannschaft ist\n",
    "    for spiele in data_test:\n",
    "        mannschaften= spiele[-2:]\n",
    "        if team in mannschaften:\n",
    "            if mannschaften[0]==team:\n",
    "                gegner= [mannschaften[1],\"away\"]\n",
    "            else:\n",
    "                gegner= [mannschaften[0],\"home\"]\n",
    "            break\n",
    "    \n",
    "    #TimeSequenz erstellen\n",
    "    for key,values in data_training.items():\n",
    "        if key!= team:\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            gegner_statistik= data_training[gegner[0]][-time_size:]\n",
    "            team_statistik= data_training[team][-time_size:]\n",
    "            #Filterung der Statistiken\n",
    "            team_statistik= get_statistik(team_statistik,team)\n",
    "            gegner_statistik= get_statistik(gegner_statistik,gegner[0])\n",
    "            #Zusammenführung und Reihenfolge beachten\n",
    "            if gegner[1]==\"away\":\n",
    "                game_series= np.concatenate((team_statistik, gegner_statistik), axis=1)\n",
    "            else:\n",
    "                game_series= np.concatenate((gegner_statistik, team_statistik), axis=1)\n",
    "            liste.append(game_series)\n",
    "            return np.array(liste)\n",
    "\n",
    "#Auxilary Daten für den Test holen\n",
    "#Einfach für ein Spiel diese Werte holen\n",
    "#Schleife läuft nicht durch, da das Spiel sich zweimal in der Liste befindet\n",
    "#output: Liste der Aux_Daten und Liste der spielenden Mannschaften\n",
    "def auxilary_data_test(data_test,team):\n",
    "    liste=[]\n",
    "    for spiele in data_test:\n",
    "        mannschaften= spiele[-2:] #Mannschaften bekommen\n",
    "        if team in mannschaften: #Mannschaft finden\n",
    "            #Werte holen\n",
    "            home_team= spiele[:19]\n",
    "            away_team= spiele[32:51]\n",
    "            odds= spiele[64:67]\n",
    "            output= spiele[67:70]\n",
    "            game_data= np.concatenate((home_team,away_team,odds))\n",
    "            liste.append(game_data)\n",
    "            return liste, mannschaften, output\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#---------------------------------SPEZIALFÄLLE-----------------------------------------------\n",
    "\n",
    "#Alle Daten, also TimeSequenz, Aux_data, Output für den 2.SPIELTAG holen\n",
    "#ist ein Spezialfall da man hier keine richtige TimeSequenz bauen kann da man nur ein Spiel\n",
    "#von der Vergangenheit kennt\n",
    "#output: Alle Daten: (TimeSequenz, Aux, Output)*2 (für beide Mannschaften), Key von Mannschaft\n",
    "def getspieltag2(data,test,mannschaftsliste,mannschaft):\n",
    "    current_team=similar(mannschaft,mannschaftsliste)\n",
    "    liste_time_series=[]\n",
    "    liste_aux=[]\n",
    "    liste_time_series_a=[]\n",
    "    liste_aux_a=[]\n",
    "    output=[]\n",
    "    output_a=[]\n",
    "    \n",
    "    #Mannschaften holen\n",
    "    for spiele in test:\n",
    "        mannschaften=spiele[-2:]\n",
    "        if mannschaftsliste.index(current_team) in mannschaften:\n",
    "            current_teams= mannschaften\n",
    "            break\n",
    "    \n",
    "    #Daten für Mannschaften holen\n",
    "    for key,values in data.items():\n",
    "        for i in current_teams:\n",
    "            if key==i and i == current_teams[0]:\n",
    "                #TimeSequenz\n",
    "                liste_time_series.append(values[0][19:32])\n",
    "                liste_time_series.append(values[0][51:64])\n",
    "                liste_aux.append(values[0][:19])\n",
    "                liste_aux.append(values[0][32:51])\n",
    "                liste_aux.append(values[0][64:67])\n",
    "                output.append(values[0][67:70])\n",
    "            if key==i and i == current_teams[1]:\n",
    "                liste_time_series_a.append(values[0][19:32])\n",
    "                liste_time_series_a.append(values[0][51:64])\n",
    "                liste_aux_a.append(values[0][:19])\n",
    "                liste_aux_a.append(values[0][32:51])\n",
    "                liste_aux_a.append(values[0][64:67])\n",
    "                output_a.append(values[0][67:70])\n",
    "                \n",
    "    liste_time_series= list(chain.from_iterable(liste_time_series))\n",
    "    liste_aux= list(chain.from_iterable(liste_aux))\n",
    "    liste_time_series_a= list(chain.from_iterable(liste_time_series_a))\n",
    "    liste_aux_a= list(chain.from_iterable(liste_aux_a))\n",
    "    \n",
    "    return np.array([[liste_time_series]]), np.array([liste_aux]),np.array(output),np.array([[liste_time_series_a]]),np.array([liste_aux_a]), np.array(output_a), mannschaftsliste.index(current_team)\n",
    "\n",
    "#---------------------SPIELTAG1\n",
    "#Hier hat man keine TIMESEQUENZEN --> Platzhalter erstellen (-1)\n",
    "#AuxDaten ganznormal holen und bei predict nochmal den selben input wie bei fit\n",
    "#output: Team1,Team2, Vorhersage\n",
    "def spieltag1(teamname,mannschaftsliste,test,d_k,d_v,n_heads,ff_dim):\n",
    "    current_team=similar(teamname,mannschaftsliste)\n",
    "    key= mannschaftsliste.index(current_team)\n",
    "    aux_data_test,mannschaften, output=auxilary_data_test(test,key)\n",
    "    aux_data= np.array(aux_data_test)\n",
    "    time= np.array([[[-1]*26]]*1) #np.full\n",
    "\n",
    "    out= output1(test,key)\n",
    "\n",
    "    model = create_model(1,aux_data,d_k,d_v,n_heads,ff_dim)\n",
    "    model.fit([time,aux_data],\n",
    "                out, \n",
    "                epochs=10, \n",
    "               )\n",
    "    train_pred = model.predict([time,aux_data]) \n",
    "    return mannschaftsliste[int(mannschaften[0])],mannschaftsliste[int(mannschaften[1])], train_pred[0], output\n",
    "        \n",
    "#Outputumformung für den ersten Spieltag\n",
    "def output1(test, mannschaft):\n",
    "    for i in test:\n",
    "        if mannschaft in i[-2:]:\n",
    "            out= i[-5:-2]\n",
    "            break\n",
    "    if out[0]==1:\n",
    "        return np.array([0])\n",
    "    elif out[1]==1:\n",
    "        return np.array([1])\n",
    "    else: \n",
    "        return np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Time2Vec hat folgende Formel: \n",
    "           (1)     w_i*r+q_i, if i=0\n",
    "    t2v(r)[i]={\n",
    "           (2)    F(w_i*r+q_i), if 1<= i <=k\n",
    "\n",
    "    r= time-series\n",
    "(1) präsentiert die nicht-periodisch/lineare Feature vom timeVektor\n",
    "    umgeschrieben = m_i*x+b_i (wie eine lineare Funktion)\n",
    "    w_i= Matrix die Steigung der time-series definiert\n",
    "    q_i= Matrix die definiert, wo time-series sich mit y-Achse schneidet\n",
    "    ==> also eine lineare Funktion\n",
    "(2) Zeile präsentiert periodischen Feature vom timeVektor\n",
    "    lineare Funktion diesmal in eine Funktion gepackt\n",
    "    Sinus Funktion erzielt beste und stabilste Ergebnisse\n",
    "    Zusammen:   q verschiebt Sinusfunktion entlang der x-Achse\n",
    "                w_i beschreibt die Wellenlänge der Sinusfunktion\n",
    "'''\n",
    "\n",
    "#Vektor wird als Keras Layer definiert\n",
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    #Erstelle 4 Matrizen, 2 für w und 2 für q, für nicht-periodisch/periodisch\n",
    "    def build(self, input_shape):\n",
    "        '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "    \n",
    "    #Eingabe hat folgende Form (batch_size,seq_len,26)\n",
    "    #batch_size: Anzahl der Sequenzen\n",
    "    #seq_len: Länge der einzelnen Sequenzen\n",
    "    #26: Anzahl der Statistiken\n",
    "    def call(self, x):\n",
    "        '''Calculate linear and periodic time features'''\n",
    "        x = tf.math.reduce_mean(x, axis=-1) \n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        #print(tf.concat([time_linear, time_periodic], axis=-1))\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hat 3 Eingaben(Abfrage, Schlüssel, Wert)\n",
    "#jede Eingabe erhält seperate lineare Transformation, indem sie einzelne dichte\n",
    "#Schichten durchläuft\n",
    "#Danach Berechnung des Aufmerksamkeitsscore/gewichte\n",
    "#Agewichte: bestimmen, wie stark Fokus bei Vorhersage eines zukünfitgen Ergebnis auf einzelne\n",
    "#Zeitserienschritte gelegt wird\n",
    "#berechnet, indem Punktprodukt der linear transformierten Abfrage/Schlüsseleingaben genommen\n",
    "#wird\n",
    "#danach dividiert durch Dimensionsgröße der vorherigen Schichten, um explodierene Gradienten\n",
    "#zuvermeiden\n",
    "#geteilte Punktprodukt durchläuft dann Softmax.\n",
    "#letzter Schritt: Softmax-Matrix wird mir transformierten v-matrix multipliziert\n",
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.key = Dense(self.d_k, \n",
    "                         input_shape=input_shape, \n",
    "                         kernel_initializer='glorot_uniform', \n",
    "                         bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.value = Dense(self.d_v, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out    \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "#Verkettung der Aufmerksamkeitsdichte von Single Head Attention Schichten\n",
    "#dann eine nichtlineare Transformation mit einer Dense-Schicht anwenden\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "        self.linear = Dense(input_shape[0][-1], \n",
    "                            input_shape=input_shape, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear   \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#Single/multi Head Attention Mechanismus nun zusammenfassen\n",
    "#jede encoder Schicht besteht aus self-attention and feedforward layer\n",
    "#feedforward besteht aus zwei dichten Schichten mit dazwischenliegender RELU Aktivierung\n",
    "'''Auf jede Teilschicht folgt eine Dropout-Schicht, nach dem Dropout wird eine Restverbindung\n",
    "gebildet, indem die anfängliche Abfrage-Eingabe zu beiden Teilschicht-Ausgängen addiert wird.\n",
    "Zum Abschluss jeder Teilschicht wird eine Normalisierungsschicht nach der \n",
    "Restverbindungsaddition platziert, um den Trainingsprozess zu stabilisieren und zu \n",
    "beschleunigen.\n",
    "Jetzt haben wir eine gebrauchsfertige Transformer-Schicht, die einfach gestapelt werden kann,\n",
    "um die Leistung eines Modells zu verbessern. Da wir keine Transformer-Decoder-Schichten \n",
    "benötigen, ist unsere implementierte Transformer-Architektur der BERT [2]-Architektur sehr \n",
    "ähnlich. Die Unterschiede sind allerdings die Zeiteinbettungen und unser Transformer \n",
    "kann eine 3-dimensionale Zeitreihe anstelle einer einfachen 2-dimensionalen Sequenz \n",
    "verarbeiten.\n",
    "\n",
    "'''\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer \n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_len,aux_data,d_k,d_v,n_heads,ff_dim):\n",
    "    '''Initialize time and transformer layers'''\n",
    "    time_embedding = Time2Vector(seq_len)\n",
    "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "    '''Construct model'''\n",
    "    units= 128\n",
    "    in_seq = Input(shape=(seq_len, 26))\n",
    "    in_aux = Input(shape=(aux_data.shape[1],))\n",
    "    x = time_embedding(in_seq)\n",
    "    x = Concatenate(axis=-1)([in_seq, x])\n",
    "    x = attn_layer1((x, x, x))\n",
    "    x = attn_layer2((x, x, x))\n",
    "    x = attn_layer3((x, x, x))\n",
    "    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    \n",
    "    a = in_aux\n",
    "    a = Dense(units, activation='relu')(a)\n",
    "    x = Concatenate(axis=1)([x,a])\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[in_seq,in_aux], outputs=out)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(spieltagsnummer,mannschaft):\n",
    "    #---------------------EINGABE----------------------\n",
    "    spieltag=spieltagsnummer\n",
    "    teamname=mannschaft\n",
    "\n",
    "    #---------------------Hyperparameter----------------\n",
    "    d_k = 128\n",
    "    d_v = 128\n",
    "    n_heads = 32\n",
    "    ff_dim = 128\n",
    "\n",
    "    #------------------- Normale Werte-------------------------------\n",
    "    training, test, mannschaftsliste= getdata(spieltag)\n",
    "    teamname_gegner,ort= getgegner(test,teamname,mannschaftsliste)\n",
    "\n",
    "    #----Spezialfälle----\n",
    "    if spieltag==1:    \n",
    "        m1,m2, predict,output= spieltag1(teamname,mannschaftsliste,test,d_k,d_v,n_heads,ff_dim)\n",
    "        print(m1,m2,predict)\n",
    "        return m1,m2,predict,output\n",
    "    \n",
    "    if spieltag==2:\n",
    "        #Training\n",
    "        time2vec, aux_data, output,time2vec_a,aux_data_a,output_a,key= getspieltag2(training,test,mannschaftsliste,teamname)\n",
    "        output_data=out_data(output)\n",
    "        output_data_a=out_data(output_a)\n",
    "        #Test\n",
    "        time2vectest= time_sequence_test(training,test,key,spieltag)\n",
    "        aux_data_test,mannschaften,output_test=auxilary_data_test(test,key)\n",
    "        seq_len=1\n",
    "        epochen=15\n",
    "        \n",
    "    #----Normalfall----\n",
    "    else:\n",
    "        #Datenreduktion, falls nötig\n",
    "        absteiger={0:[9,14],1:[7,8]}\n",
    "        aufsteiger={1:[18,19],2:[9,14]}\n",
    "        training_neu,spieltag_neu= data_auf_absteiger(training,mannschaftsliste,[teamname,teamname_gegner],spieltag,absteiger,aufsteiger)\n",
    "        if training_neu==\"KEY ERROR\":\n",
    "            return \"Fehler: Mannschaft befindet sich nicht in der Saison\"\n",
    "        \n",
    "        #Datenvorbereitung\n",
    "        if spieltag_neu==1:\n",
    "            m1,m2, predict,output_test= spieltag1(teamname,mannschaftsliste,test,d_k,d_v,n_heads,ff_dim)\n",
    "            print(m1,m2,predict)\n",
    "            return m1,m2,predict, output_test\n",
    "        elif spieltag_neu==2:\n",
    "            #Training\n",
    "            time2vec, aux_data, output,time2vec_a,aux_data_a,output_a,key= getspieltag2(training,test,mannschaftsliste,teamname)\n",
    "            output_data=out_data(output)\n",
    "            output_data_a=out_data(output_a)\n",
    "            #Test\n",
    "            time2vectest= time_sequence_test(training,test,key,spieltag)\n",
    "            aux_data_test,mannschaften,output_test=auxilary_data_test(test,key)\n",
    "            seq_len=1\n",
    "            epochen=15\n",
    "        else:\n",
    "            #Transformer Werte\n",
    "            #Home\n",
    "            time2vec,key, seq_len= time_sequence(training_neu,mannschaftsliste,teamname,spieltag_neu)\n",
    "            aux_data, output= auxilary_data(training_neu,key,spieltag_neu)\n",
    "            aux_data= np.array(aux_data[0])\n",
    "            output_data= out_data(output[0])\n",
    "\n",
    "            #Away\n",
    "            time2vec_a,key_a,seq_len= time_sequence(training_neu,mannschaftsliste,teamname_gegner,spieltag_neu)\n",
    "            aux_data_a,output_a= auxilary_data(training_neu,key_a,spieltag_neu)\n",
    "            aux_data_a=np.array(aux_data_a[0])\n",
    "            output_data_a= out_data(output_a[0])\n",
    "\n",
    "            #Test\n",
    "            time2vectest= time_sequence_test(training_neu,test,key,spieltag)\n",
    "            aux_data_test,mannschaften,output_test=auxilary_data_test(test,key)\n",
    "            epochen=50\n",
    "    #Zusammenführung der Daten in richtiger Reihenfolge\n",
    "    if ort==\"home\":\n",
    "        time_all=np.concatenate((time2vec,time2vec_a),axis=0)\n",
    "        aux_all= np.concatenate((aux_data,aux_data_a),axis=0)\n",
    "        out_all= np.concatenate((output_data,output_data_a),axis=0)\n",
    "    else:\n",
    "        time_all=np.concatenate((time2vec_a,time2vec),axis=0)\n",
    "        aux_all= np.concatenate((aux_data_a,aux_data),axis=0)\n",
    "        out_all= np.concatenate((output_data_a,output_data),axis=0)\n",
    "    #--------------------------- Transformer ausfuehren----------------------\n",
    "    model = create_model(seq_len,aux_data,d_k,d_v,n_heads,ff_dim)\n",
    "    \n",
    "    filepath= \"Transformer.hdf5\"\n",
    "    checkpoint_dir = os.path.dirname(filepath)\n",
    "\n",
    "    model.fit([time_all,aux_all],\n",
    "                        out_all, \n",
    "                        epochs=epochen, \n",
    "                       )\n",
    "\n",
    "    train_pred = model.predict([time2vectest,np.array(aux_data_test)]) \n",
    "    print(mannschaftsliste[int(mannschaften[0])],mannschaftsliste[int(mannschaften[1])])\n",
    "    print(train_pred)\n",
    "    if ort==\"home\":\n",
    "        return [teamname,teamname_gegner,train_pred,output_test]\n",
    "    else:\n",
    "        return [teamname_gegner,teamname,train_pred,output_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borussia Dortmund\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 42s 42s/step - loss: 0.8770 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7661 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5837 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5480 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4439 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3352 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2605 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1963 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1884 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1446 - accuracy: 1.0000\n",
      "Borussia Dortmund FC Augsburg [0.898043   0.05684274 0.0451143 ]\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 42s 42s/step - loss: 1.4840 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3708 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2027 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9867 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9179 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7942 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7124 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6752 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4812 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4701 - accuracy: 1.0000\n",
      "Borussia Dortmund FC Augsburg [0.6731111  0.18130846 0.14558043]\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 41s 41s/step - loss: 0.8510 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7392 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5859 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4359 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4272 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3219 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1564 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1298 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0988 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0928 - accuracy: 1.0000\n",
      "Borussia Dortmund FC Augsburg [0.94170475 0.03452182 0.02377343]\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "def get_mannschaften():\n",
    "    data=pd.read_pickle(\"Daten2020.pkl\")\n",
    "    mannschaften= data[2]\n",
    "    return mannschaften\n",
    "\n",
    "def evaluate():\n",
    "    teams= get_mannschaften()\n",
    "    sammlung_mannschaften= {}\n",
    "    #'print(teams[:18])\n",
    "    for mannschaft in teams:\n",
    "        print(mannschaft)\n",
    "        sammlung_spieltage= {}\n",
    "        for spieltag in range(1,35):\n",
    "            liste=[]\n",
    "            liste.append(main(spieltag,mannschaft))\n",
    "            liste.append(main(spieltag,mannschaft))\n",
    "            liste.append(main(spieltag,mannschaft))\n",
    "            sammlung_spieltage[spieltag]= liste\n",
    "        sammlung_mannschaften[mannschaft]= sammlung_spieltage\n",
    "    with open('ErgebnisseTransformer.pkl','wb') as f: \n",
    "        pickle.dump(sammlung_mannschaften, f)\n",
    "\n",
    "ergebnisse= evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data_set, periods):\n",
    "    weights = np.ones(periods) / periods\n",
    "    return np.convolve(data_set, weights, mode='valid')\n",
    "\n",
    "\n",
    "def time_sequence_roll(data,mannschaftsliste,mannschaft,spieltag,roll):\n",
    "    \n",
    "    time_size, anzahl= size_of_time(spieltag)\n",
    "    time_series={}\n",
    "    list_of_game_series=[]\n",
    "    list_of_game_series_val=[]\n",
    "    grenze1= (spieltag-1)-(int(spieltag/8))\n",
    "    grenze2= spieltag-1\n",
    "    #Mannschaftnummer finden\n",
    "    current_team=similar(mannschaft,mannschaftsliste)\n",
    "    \n",
    "    #rolling average hinzufuegen\n",
    "    time_size+= roll-1\n",
    "    for key,values in data.items():\n",
    "        #start= spieltag-time_size-2\n",
    "                \n",
    "        if key!= mannschaftsliste.index(current_team):\n",
    "            continue\n",
    "        else:\n",
    "            for i in range(time_size,grenze1): #Spiele die betrachtet werden\n",
    "                teams= values[i][-2:]\n",
    "                start_gegner= i-time_size\n",
    "                #print(i,start_gegner)\n",
    "            \n",
    "                if teams[0]==key:\n",
    "                    #\"Rohe\" Daten holen\n",
    "                    gegner_statistik= data[teams[1]][start_gegner:i]\n",
    "                    team_statistik= data[teams[0]][start_gegner:i]\n",
    "                    #Filterung der Statistiken\n",
    "                    team_statistik= get_statistik(team_statistik,teams[0])\n",
    "                    gegner_statistik= get_statistik(gegner_statistik,teams[1])\n",
    "                    #Zusammenführung\n",
    "                    game_series= np.concatenate((team_statistik, gegner_statistik), axis=1)\n",
    "                else:\n",
    "                    #\"Rohe\" Daten holen\n",
    "                    gegner_statistik= data[teams[0]][start_gegner:i]\n",
    "                    team_statistik= data[teams[1]][start_gegner:i]              \n",
    "                    #Filterung der Statistiken\n",
    "                    team_statistik= get_statistik(team_statistik,teams[1])\n",
    "                    gegner_statistik= get_statistik(gegner_statistik,teams[0])\n",
    "                    #Zusammenführung\n",
    "                    game_series= np.concatenate((gegner_statistik, team_statistik), axis=1)\n",
    "                #print(game_series)\n",
    "                rolling_average= pd.DataFrame(game_series).rolling(5).mean()\n",
    "                rolling_average= rolling_average.dropna()\n",
    "                list_of_game_series.append(rolling_average.to_numpy())\n",
    "                \n",
    "            for i in range(grenze1+roll-1,grenze2): #Spiele die betrachtet werden\n",
    "                teams= values[i][-2:]\n",
    "                start_gegner= i-time_size\n",
    "                #print(i,start_gegner)\n",
    "            \n",
    "                if teams[0]==key:\n",
    "                    #\"Rohe\" Daten holen\n",
    "                    gegner_statistik= data[teams[1]][start_gegner:i]\n",
    "                    team_statistik= data[teams[0]][start_gegner:i]\n",
    "                    #Filterung der Statistiken\n",
    "                    team_statistik= get_statistik(team_statistik,teams[0])\n",
    "                    gegner_statistik= get_statistik(gegner_statistik,teams[1])\n",
    "                    #Zusammenführung\n",
    "                    game_series= np.concatenate((team_statistik, gegner_statistik), axis=1)\n",
    "                else:\n",
    "                    #\"Rohe\" Daten holen\n",
    "                    gegner_statistik= data[teams[0]][start_gegner:i]\n",
    "                    team_statistik= data[teams[1]][start_gegner:i]              \n",
    "                    #Filterung der Statistiken\n",
    "                    team_statistik= get_statistik(team_statistik,teams[1])\n",
    "                    gegner_statistik= get_statistik(gegner_statistik,teams[0])\n",
    "                    #Zusammenführung\n",
    "                    game_series= np.concatenate((gegner_statistik, team_statistik), axis=1)\n",
    "                #print(game_series)\n",
    "                rolling_average= pd.DataFrame(game_series).rolling(5).mean()\n",
    "                rolling_average= rolling_average.dropna()\n",
    "                list_of_game_series_val.append(rolling_average.to_numpy())\n",
    "    mannschaftskey= mannschaftsliste.index(current_team)\n",
    "    return np.array(list_of_game_series), mannschaftskey, np.array(list_of_game_series_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 5, 26) (7, 5, 26) (1, 5, 26)\n",
      "(73,) (7,)\n",
      "(73, 41) (7, 41) (1, 41)\n",
      "Epoch 1/35\n",
      "3/3 [==============================] - 19s 2s/step - loss: 0.4487 - mae: 0.5770 - mape: 6189412.3750 - val_loss: 0.4168 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41677, saving model to Transformer_roll.hdf5\n",
      "Epoch 2/35\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.4654 - mae: 0.5983 - mape: 6189411.8750 - val_loss: 0.4171 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.41677\n",
      "Epoch 3/35\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.4625 - mae: 0.5859 - mape: 6189410.2500 - val_loss: 0.4171 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.41677\n",
      "Epoch 4/35\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4558 - mae: 0.5843 - mape: 6189410.8750 - val_loss: 0.4163 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.41677 to 0.41627, saving model to Transformer_roll.hdf5\n",
      "Epoch 5/35\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.4226 - mae: 0.5608 - mape: 2283160.5933 - val_loss: 0.4161 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41627 to 0.41605, saving model to Transformer_roll.hdf5\n",
      "Epoch 6/35\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4433 - mae: 0.5749 - mape: 6189410.6250 - val_loss: 0.4161 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.41605\n",
      "Epoch 7/35\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4422 - mae: 0.5696 - mape: 3585243.7648 - val_loss: 0.4160 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.41605 to 0.41604, saving model to Transformer_roll.hdf5\n",
      "Epoch 8/35\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4293 - mae: 0.5665 - mape: 3585243.8615 - val_loss: 0.4160 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.41604 to 0.41595, saving model to Transformer_roll.hdf5\n",
      "Epoch 9/35\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.4365 - mae: 0.5712 - mape: 3585243.6445 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41595 to 0.41595, saving model to Transformer_roll.hdf5\n",
      "Epoch 10/35\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4531 - mae: 0.5790 - mape: 3585243.7201 - val_loss: 0.4160 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41595\n",
      "Epoch 11/35\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4351 - mae: 0.5670 - mape: 6189410.0000 - val_loss: 0.4160 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41595\n",
      "Epoch 12/35\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4684 - mae: 0.5920 - mape: 6189410.8750 - val_loss: 0.4160 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41595\n",
      "Epoch 13/35\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4213 - mae: 0.5577 - mape: 6189409.7500 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.41595\n",
      "Epoch 14/35\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4584 - mae: 0.5868 - mape: 3585245.2036 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.41595 to 0.41593, saving model to Transformer_roll.hdf5\n",
      "Epoch 15/35\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4296 - mae: 0.5686 - mape: 6189411.0000 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.41593 to 0.41592, saving model to Transformer_roll.hdf5\n",
      "Epoch 16/35\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4371 - mae: 0.5733 - mape: 6189410.0000 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.41592 to 0.41591, saving model to Transformer_roll.hdf5\n",
      "Epoch 17/35\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4475 - mae: 0.5858 - mape: 6189411.0000 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.41591\n",
      "Epoch 18/35\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4487 - mae: 0.5717 - mape: 6189409.5000 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.41591\n",
      "Epoch 19/35\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4754 - mae: 0.6030 - mape: 6189412.0000 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.41591\n",
      "Epoch 20/35\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4391 - mae: 0.5696 - mape: 3585243.2483 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41591\n",
      "Epoch 21/35\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4504 - mae: 0.5749 - mape: 6189409.8750 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.41591 to 0.41591, saving model to Transformer_roll.hdf5\n",
      "Epoch 22/35\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.4662 - mae: 0.5858 - mape: 6189410.6250 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.41591 to 0.41590, saving model to Transformer_roll.hdf5\n",
      "Epoch 23/35\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.4566 - mae: 0.5837 - mape: 3585243.9913 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41590\n",
      "Epoch 24/35\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.4525 - mae: 0.5858 - mape: 6189410.6250 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41590\n",
      "Epoch 25/35\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4316 - mae: 0.5655 - mape: 6189409.7500 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.41590\n",
      "Epoch 26/35\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4483 - mae: 0.5827 - mape: 6189410.7500 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.41590\n",
      "Epoch 27/35\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4483 - mae: 0.5775 - mape: 3585244.0903 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.41590\n",
      "Epoch 28/35\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.4729 - mae: 0.5978 - mape: 3585245.5291 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.41590\n",
      "Epoch 29/35\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4395 - mae: 0.5775 - mape: 3585244.5291 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.41590\n",
      "Epoch 30/35\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4612 - mae: 0.5900 - mape: 3585244.5668 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.41590\n",
      "Epoch 31/35\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4445 - mae: 0.5775 - mape: 3585244.1871 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.41590\n",
      "Epoch 32/35\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4453 - mae: 0.5743 - mape: 3585243.9653 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.41590\n",
      "Epoch 33/35\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4503 - mae: 0.5795 - mape: 6189410.5000 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.41590\n",
      "Epoch 34/35\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4499 - mae: 0.5853 - mape: 3585244.2674 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.41590\n",
      "Epoch 35/35\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4733 - mae: 0.6014 - mape: 6189411.8750 - val_loss: 0.4159 - val_mae: 0.5810 - val_mape: 58.3333\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.41590 to 0.41590, saving model to Transformer_roll.hdf5\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fa4cbed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.33603513 0.3412453  0.3227195 ]]\n"
     ]
    }
   ],
   "source": [
    "spieltag=94\n",
    "teamname=\"München\"\n",
    "\n",
    "#---------------------Hyperparameter----------------\n",
    "batch_size = 32\n",
    "d_k = 128\n",
    "d_v = 128\n",
    "n_heads = 12\n",
    "ff_dim = 128\n",
    "#-----------------------------Rolling Average---------------------------\n",
    "roll=5\n",
    "time2vec_roll,key,time2vec_roll_val= time_sequence_roll(training,mannschaftsliste,teamname,spieltag,roll)\n",
    "output_data_roll= moving_average(output_data,roll)\n",
    "output_data_roll_val= moving_average(output_data_val,roll)\n",
    "aux_data, output,aux_data_val,output_val= auxilary_data(training,key)\n",
    "aux_data= np.array(aux_data[0])[roll-1:]\n",
    "aux_data_val= np.array(aux_data_val[0])[roll-1:]\n",
    "aux_data_test=auxilary_data_test(test,key)\n",
    "\n",
    "print(time2vec_roll.shape,time2vec_roll_val.shape,time2vectest.shape)\n",
    "print(output_data_roll.shape,output_data_roll_val.shape)\n",
    "print(aux_data.shape,aux_data_val.shape, np.array(aux_data_test).shape)\n",
    "\n",
    "#--------------------------- Transformer ausfuehren----------------------\n",
    "\n",
    "model = create_model()\n",
    "#model.summary()\n",
    "filepath= \"Transformer_roll.hdf5\"\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                              monitor='val_loss', \n",
    "                                              save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit([time2vec_roll,aux_data], output_data_roll, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=35, \n",
    "                    callbacks=[callback],\n",
    "                    validation_data=([time2vec_roll_val,aux_data_val], output_data_roll_val)\n",
    "                   )\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(filepath,\n",
    "                                   custom_objects={'Time2Vector': Time2Vector, \n",
    "                                                   'SingleAttention': SingleAttention,\n",
    "                                                   'MultiAttention': MultiAttention,\n",
    "                                                   'TransformerEncoder': TransformerEncoder})\n",
    "\n",
    "\n",
    "\n",
    "train_pred = model.predict([time2vectest,np.array(aux_data_test)])\n",
    "print(train_pred)\n",
    "#train_eval = model.evaluate(time2vec, output_data, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
